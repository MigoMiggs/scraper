{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a1d54a-c807-4af9-b2f2-abb81e88d702",
   "metadata": {},
   "source": [
    "# IBTS Production Grade RAG\n",
    "This notebooks shows step by step how we get the right answers from the knowledge base. The notebook is meant to be educational showiong a few different possibilities that one can take to tune the system.\n",
    "\n",
    "## Step 1 - Do all the setup, imports,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9a428564-d04d-4bad-ba0f-1930c85e1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.load import dumps, loads\n",
    "from langchain import hub\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate, \n",
    "    ChatPromptTemplate\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import (RetrievalQA, \n",
    "    RetrievalQAWithSourcesChain\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698705a6-ea27-4542-9a0b-d0a6f061bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8a8dcdea-e023-4035-915c-31acb6552765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model object\n",
    "llm_model = \"gpt-3.5-turbo-16k\"\n",
    "llm = ChatOpenAI(model_name=llm_model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14216cf0-499c-4597-bfc8-c926c2bcda78",
   "metadata": {},
   "source": [
    "## Step 2 Load the DB and setup the plain vanilla retriever\n",
    "The plain vanilla retriver will just fetch the most similar chunks from the embedded dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e51a28-5a72-4c19-a4f6-160b47cfd97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chroma DB\n",
    "persist_directory = '../chroma_clean_ada/'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226412c-bca9-4864-ad8e-b72be3cac1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of document chunks in the DB\n",
    "print(f\"Total chunks: {vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ee762-bb21-4b86-a8de-2d63abb9d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make siure that our retriever gets back 6 results\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a3df9-9e0a-4dac-a41d-1304f4a1d05b",
   "metadata": {},
   "source": [
    "## Step 3 - Run a query with simple retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5f970-40c8-4c88-8b52-cd6c3427d05b",
   "metadata": {},
   "source": [
    "Set up the simple QA chanin with just the plain vanilla retriver to see what we get out of the bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65d909-0255-4e31-a96b-fab2a1e4f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2ca70-c1cd-4ed1-b6ed-c983328edc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "query  = \"what are the key aspects of the orlando budget guide?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87efef95-1419-4680-b9a7-3919b35b2a66",
   "metadata": {},
   "source": [
    "## Step 4 - Meta prompt utilizing questions form the City Of Orlando Assessment \n",
    "\n",
    "Here we are going to put the multiple question guidance in the system prompt and will instruct GPT how to handle them and the answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "116a7d0c-3a1d-4557-a636-64c7033ccce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### System Prompt Construction\n",
    "\n",
    "system_template = \"\"\"\"You are an evluator that knows about Natural Disaster Recovery. \\\n",
    "User will provider a multiple choice question and the possible answers below. You will pick the best answer based on the \\\n",
    "following pieces of context. The questions will always go from 1 - 6, the 6th answer is always \"I don't know.\" answers 1 - 5 \\\n",
    "will go from low to high, from the perspecive of how good the adherence is to the provided question. These questions and answers\n",
    "are used for Natural Disaster Readiness. The Community Resilience Assessment Framework and Tools (CRAFT) \\\n",
    "Equitable Climate Resilience (ECR) platform is a resource for cities to assess and strengthen their resilience - \\\n",
    "the ability to to mitigate, respond to, and recover from crises. Also, after the answer, you will explain how you got to the answer, \\ \n",
    "referrring to the pieces of context that gave you the answer. \\n\\\n",
    "-------------------- \\n\\\n",
    "{context}\n",
    "\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bc9a47da-0ad7-470a-996e-81bae523c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User Prompt\n",
    "#### Question 1A from the Orlando-PreAssessment\n",
    "\n",
    "query = \"To what extent is the relationship between climate hazards and \\\n",
    "social vulnerability/inequity understood among city leaders and staff? \\n\\n\"\n",
    "\n",
    "answers = \"\\\n",
    "Possible Answers: \\n\\\n",
    "1 (Low) The relationship between climate hazards and social inequity has not been explored by staff or elected officials \\n\\\n",
    "2 (LowMid) \\n\\\n",
    "3 (Medium) The relationship between climate hazards and social inequity is familiar to select city staff or elected \\n\\\n",
    "officials \\n\\\n",
    "4 (MidHigh) \\n\\\n",
    "5 (High) City staff and elected officials are well-versed in the concepts and taxonomy of the relationship between climate hazards and social inequity \\n\\\n",
    "6 I dont know \\n \"\n",
    "\n",
    "query += answers\n",
    "\n",
    "human_template=\"\"\" {question} \"\"\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1ff77027-f47a-42b6-a02b-4a4621f60dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "ChatPromptTemplate.input_variables=[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d0bd87f4-cff2-4d98-a205-afe633c51e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To what extent is the relationship between climate hazards and social vulnerability/inequity understood among city leaders and staff? \n",
      "\n",
      "Possible Answers: \n",
      "1 (Low) The relationship between climate hazards and social inequity has not been explored by staff or elected officials \n",
      "2 (LowMid) \n",
      "3 (Medium) The relationship between climate hazards and social inequity is familiar to select city staff or elected \n",
      "officials \n",
      "4 (MidHigh) \n",
      "5 (High) City staff and elected officials are well-versed in the concepts and taxonomy of the relationship between climate hazards and social inequity \n",
      "6 I dont know \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "29931dc8-5111-45ce-89bd-fce96430d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": chat_prompt\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "21a0a291-76be-404b-a155-4d7ebe9d2cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would choose answer 3 (Medium) - The relationship between climate hazards and social inequity is familiar to select city staff or elected officials.\n",
      "\n",
      "Based on the provided context, it is mentioned that in the City's Climate Vulnerability Assessment, completed in 2017, the relationship between hazards and risks from the changing climate, as well as demographics such as children, the ill, and the elderly, is investigated. This suggests that there is some level of understanding among city staff or elected officials regarding the relationship between climate hazards and social vulnerability/inequity.\n",
      "\n",
      "However, it is not explicitly stated that all city staff or elected officials are well-versed in these concepts, indicating that the understanding may be limited to select individuals. Therefore, answer 3 (Medium) is the most appropriate choice.\n"
     ]
    }
   ],
   "source": [
    "#query  = \"what are the key aspects of the orlando budget guide?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a262642c-6917-4772-a189-8f6f7f170172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have potential barriers to the participation of vulnerable populations in the planning and implementation process been identified for the City Of Orlando? \n",
      "\n",
      "Possible answers: \n",
      "1 (Low) Potential barriers have not been studied or identified \n",
      "2 (LowMid) \n",
      "3 (Medium) Potential barriers have been identified, and plans to reduce barriers to participation are underway \n",
      "4 (MidHigh) \n",
      "5 (High) Barriers have been identified and the city has taken corrective action to reduce these barriers \n"
     ]
    }
   ],
   "source": [
    "##### Run another question to see the results  ######\n",
    "#### Question 5d from the Orlando-PreAssessment\n",
    "\n",
    "query = \"Have potential barriers to the participation of vulnerable populations in \\\n",
    "the planning and implementation process been identified for the City Of Orlando? \\n\\n\"\n",
    "\n",
    "answers = \"Possible answers: \\n\\\n",
    "1 (Low) Potential barriers have not been studied or identified \\n\\\n",
    "2 (LowMid) \\n\\\n",
    "3 (Medium) Potential barriers have been identified, and plans to reduce barriers to participation are underway \\n\\\n",
    "4 (MidHigh) \\n\\\n",
    "5 (High) Barriers have been identified and the city has taken corrective action to reduce these barriers \\\n",
    "\"\n",
    "\n",
    "query += answers\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fa3bfc26-8891-4996-851d-556c437441c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 (MidHigh) \n",
      "\n",
      "Explanation: Based on the provided context, it is mentioned that the City of Orlando strongly encourages the participation of the entire community in the planning and implementation process. This indicates that potential barriers to the participation of vulnerable populations have been identified. However, the context does not explicitly state that corrective action has been taken to reduce these barriers. Therefore, the answer would be 4 (MidHigh), indicating that barriers have been identified, but it is unclear if corrective action has been taken.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7796d7-489c-4551-8e82-16543b2bfb20",
   "metadata": {},
   "source": [
    "## Step 5 - Compare results with using a MultiQueryRetriever\n",
    "\n",
    "We are going to try to get even better results using a multi query retriiver, it may not affect the answer from multiple choice, but it will affect the explanation for the answer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f5f27656-9ebd-4a17-a8bb-97bb4da7e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\":6}), llm=llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e56d3d2b-338f-4b06-8086-cbe14e89cb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 (High) Barriers have been identified and the city has taken corrective action to reduce these barriers.\n",
      "\n",
      "Explanation: The context mentions that the Resilience Plan includes steps to gain internal and external feedback on resilience strategies. This includes conducting a Baseline Assessment and an Updated Vulnerability Assessment, which would likely involve identifying potential barriers to participation. Additionally, the context states that the plan aims to create a more equitable city and ensure that all persons have the resources and support to bounce back in the face of adversity. This suggests that the city is actively working to reduce barriers to participation for vulnerable populations.\n"
     ]
    }
   ],
   "source": [
    "###### Use the same query as above\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": chat_prompt\n",
    "    }\n",
    ")\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc915fb5-2278-4eef-992e-cccb9344f1dc",
   "metadata": {},
   "source": [
    "## Step 6 - Go next level with Rank Fusion\n",
    "\n",
    "This is something taken from this repo originally: https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "375556bf-e125-474a-a98a-60768a6deb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull('langchain-ai/rag-fusion-query-generation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "87e235d7-b71e-42ac-9a5a-24da3b77b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful assistant that generates multiple search queries based on a single input query.\"),\n",
    "#     (\"user\", \"Generate multiple search queries related to: {original_query}\"),\n",
    "#     (\"user\", \"OUTPUT (4 queries):\")\n",
    "# ])\n",
    "generate_queries = prompt | ChatOpenAI(temperature=0) | StrOutputParser() | (lambda x: x.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9c6072ba-d640-418b-93f9-6c929f29e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # Assumes the docs are returned in sorted order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "            \n",
    "    reranked_results = [(loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)]\n",
    "    return reranked_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e7533d55-6120-4a40-838c-ef98112611f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = chain.invoke({\"original_query\": query})\n",
    "\n",
    "doc_list = []\n",
    "for d in docs:\n",
    "    for dd in d:\n",
    "        if hasattr(dd, 'page_content'):\n",
    "            doc_list.append(dd)\n",
    "     \n",
    "\n",
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3e112bd7-3e94-45d7-aa47-9919793d4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_fusion_db = Chroma.from_documents(doc_list, embeddings)\n",
    "retriever=rank_fusion_db.as_retriever(search_kwargs={\"k\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1c957301-f882-4d45-8b1a-5788bd7917cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 (High) Barriers have been identified and the city has taken corrective action to reduce these barriers.\n",
      "\n",
      "Explanation: The context provided mentions that the City of Orlando has conducted workshops with community leaders to discuss concerns and opportunities for protecting and recovering from challenges. Additionally, the city has conducted a baseline assessment and updated vulnerability assessment to identify existing best practices and broaden the conversation on vulnerability, including concerns such as affordable housing. These actions indicate that the city has actively identified potential barriers to participation and has taken steps to reduce these barriers.\n"
     ]
    }
   ],
   "source": [
    "###### Use the same query as above\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": chat_prompt\n",
    "    }\n",
    ")\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f67ea232-0b4b-4f08-b5a5-ccba919dd638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To what extent are Equitable Climate Resilience related priorities/projects coordinated with regional jurisdictions (e.g., city, county, state, districts, etc.)? \n",
      "Possible answers: \n",
      "1 (Low)  The city does not coordinate with regional jurisdictions \n",
      "2 (LowMid) \n",
      "3 (Medium) The city informally coordinates with select regional jurisdictions \n",
      "4 (MidHigh) \n",
      "5 (High) The city has an established network and forum to coordinate with regional jurisdictions \n"
     ]
    }
   ],
   "source": [
    "##### Run another question to see the results  ######\n",
    "#### Question 5a from the Orlando-PreAssessment\n",
    "\n",
    "query = \"To what extent are ECR-related priorities/projects coordinated with regional \\\n",
    "jurisdictions (e.g., city, county, state, districts, etc.)? \\n\"\n",
    "\n",
    "answers = \"Possible answers: \\n\\\n",
    "1 (Low)  The city does not coordinate with regional jurisdictions \\n\\\n",
    "2 (LowMid) \\n\\\n",
    "3 (Medium) The city informally coordinates with select regional jurisdictions \\n\\\n",
    "4 (MidHigh) \\n\\\n",
    "5 (High) The city has an established network and forum to coordinate with regional jurisdictions \\\n",
    "\"\n",
    "\n",
    "query += answers\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "91c21887-6c1b-49d0-914b-02d80c973bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = chain.invoke({\"original_query\": query})\n",
    "\n",
    "doc_list = []\n",
    "for d in docs:\n",
    "    for dd in d:\n",
    "        if hasattr(dd, 'page_content'):\n",
    "            doc_list.append(dd)\n",
    "     \n",
    "\n",
    "len(doc_list)\n",
    "\n",
    "rank_fusion_db = Chroma.from_documents(doc_list, embeddings)\n",
    "retriever=rank_fusion_db.as_retriever(search_kwargs={\"k\":10})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": chat_prompt\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "526d0063-96d2-4d4e-9fe6-176f2ae134b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 (High) The city has an established network and forum to coordinate with regional jurisdictions.\n",
      "\n",
      "Explanation: The provided context states that the City of Orlando challenges other local jurisdictions, the East Central Florida Regional Planning Council, and the State of Florida to ensure proper coordination for the growth of Central Florida. This indicates that the city actively engages with regional jurisdictions and has established networks and forums for coordination. Therefore, the answer is 5 (High).\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e4ffa-fa1b-4c31-aeb6-1cb3a74bafbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
