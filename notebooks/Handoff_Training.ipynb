{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a1d54a-c807-4af9-b2f2-abb81e88d702",
   "metadata": {},
   "source": [
    "# IBTS Production Grade RAG\n",
    "This notebooks shows step by step how we get the right answers from the knowledge base. The notebook is meant to be educational showiong a few different possibilities that one can take to tune the system.\n",
    "\n",
    "## Step 1 - Do all the setup, imports,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a428564-d04d-4bad-ba0f-1930c85e1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.load import dumps, loads\n",
    "from langchain import hub\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate, \n",
    "    ChatPromptTemplate\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import (RetrievalQA, \n",
    "    RetrievalQAWithSourcesChain\n",
    ")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#openai.api_key = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698705a6-ea27-4542-9a0b-d0a6f061bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8dcdea-e023-4035-915c-31acb6552765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gptmodel(use_azure, model_name):\n",
    "\n",
    "    llm_model = None\n",
    "    if use_azure:\n",
    "        llm_model = AzureChatOpenAI(\n",
    "            openai_api_base=\"https://sondertest.openai.azure.com/\",\n",
    "            openai_api_version=\"2023-07-01-preview\",\n",
    "            deployment_name=\"dev-gpt-35-turbo-16k\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_AZURE_KEY\"),\n",
    "            openai_api_type=\"azure\",\n",
    "            temperature=0 \n",
    "            )\n",
    "    else: \n",
    "        llm_model = ChatOpenAI(model_name=model_name, temperature=0)\n",
    "\n",
    "    return llm_model\n",
    "\n",
    "\n",
    "# Instantiate the model object\n",
    "llm_model = \"gpt-3.5-turbo-16k\"\n",
    "llm = get_gptmodel(True, llm_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14216cf0-499c-4597-bfc8-c926c2bcda78",
   "metadata": {},
   "source": [
    "## Step 2 Load the DB and setup the plain vanilla retriever\n",
    "The plain vanilla retriver will just fetch the most similar chunks from the embedded dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e51a28-5a72-4c19-a4f6-160b47cfd97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chroma DB\n",
    "persist_directory = '../chroma_clean_ada/'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2226412c-bca9-4864-ad8e-b72be3cac1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 120547\n"
     ]
    }
   ],
   "source": [
    "# Display the number of document chunks in the DB\n",
    "print(f\"Total chunks: {vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "308ee762-bb21-4b86-a8de-2d63abb9d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make siure that our retriever gets back 6 results\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":7})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a3df9-9e0a-4dac-a41d-1304f4a1d05b",
   "metadata": {},
   "source": [
    "## Step 3 - Run a query with simple retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5f970-40c8-4c88-8b52-cd6c3427d05b",
   "metadata": {},
   "source": [
    "Set up the simple QA chanin with just the plain vanilla retriver to see what we get out of the bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c65d909-0255-4e31-a96b-fab2a1e4f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b2ca70-c1cd-4ed1-b6ed-c983328edc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key aspects of the Orlando budget guide include:\n",
      "\n",
      "1) Policy Guide: The budget serves as a policy document that informs the reader about the municipality and its policies. It includes organization-wide financial and programmatic policies and goals that address long-term concerns and issues, as well as short-term financial and operational policies that guide the development of the annual budget. The department budget sections provide mission statements, major accomplishments, future outlook (goals), and performance indicators for each department.\n",
      "\n",
      "2) Financial Plan: The budget details the costs associated with providing municipal services and how these services will be funded. It includes a summary and detailed description of all revenues and expenditures. The budget serves as a financial plan that outlines the financial resources available to the city and how they will be allocated to meet the needs of the community.\n",
      "\n",
      "3) Mayor's Key Priorities: The budget focuses on the Mayor's key priorities, which serve as a roadmap for achieving the city's mission. These priorities include creating a city for everyone, creating high-quality jobs, ending homelessness, keeping the community safe, becoming one of the most sustainable cities in America, and providing mobility and transportation options. The budget allocates resources and funding to support initiatives and programs related to these priorities.\n",
      "\n",
      "4) Financial Responsibility: The budget reflects the city's commitment to financial responsibility. Despite challenging economic conditions, the budget aims to ensure that the city government lives within its means while still providing superior services to the community. The budget document highlights the choices and decisions made by the city's elected leaders to maintain the city's financial stability.\n",
      "\n",
      "Please note that the specific details and priorities may vary depending on the fiscal year mentioned in the context.\n"
     ]
    }
   ],
   "source": [
    "query  = \"what are the key aspects of the orlando budget guide?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87efef95-1419-4680-b9a7-3919b35b2a66",
   "metadata": {},
   "source": [
    "## Step 4 - Meta prompt utilizing questions form the City Of Orlando Assessment \n",
    "\n",
    "Here we are going to put the multiple question guidance in the system prompt and will instruct GPT how to handle them and the answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a7d0c-3a1d-4557-a636-64c7033ccce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### System Prompt Construction\n",
    "\n",
    "system_template = \"\"\"\" \\\n",
    "Instructions:\n",
    "* You are an evaluator that knows about Natural Disaster Recovery. \\\n",
    "* User will provider a multiple choice question and the possible answers below. \\\n",
    "* You will pick the best answer based on the included pieces of context. The questions \\\n",
    "will always go from 1 - 6, the 6th answer is always \"I don't know.\" \n",
    "* Answers 1 - 5 will go from low to high, from the perspecive of how good \\\n",
    "the adherence is to the provided question. \n",
    "* These questions and answers are used for Natural Disaster Readiness. \\\n",
    "* The Community Resilience Assessment Framework and Tools (CRAFT) \\\n",
    "* Equitable Climate Resilience (ECR) platform is a resource for cities to assess and strengthen their resilience - \\\n",
    "the ability to to mitigate, respond to, and recover from crises. Also, after the answer, you will explain how you got to the answer, \\ \n",
    "referrring to the pieces of context that gave you the answer. \\n\\\n",
    "-------------------- \\n\\\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_template=\"\"\" {question} \"\"\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd719370-7102-40f0-ae93-a92a7010dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "#ChatPromptTemplate.input_variables=[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa65bab9-4440-4a18-b706-2696fe6697eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": chat_prompt\n",
    "    }, \n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a47da-0ad7-470a-996e-81bae523c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User Prompt\n",
    "#### Question 1A from the Orlando-PreAssessment\n",
    "\n",
    "query = \"To what extent is the relationship between climate hazards and \\\n",
    "social vulnerability/inequity understood among city leaders and staff? \\n\\n\"\n",
    "\n",
    "answers = \"\\\n",
    "Possible Answers: \\n\\\n",
    "1 (Low) The relationship between climate hazards and social inequity has not been explored by staff or elected officials \\n\\\n",
    "2 (LowMid) \\n\\\n",
    "3 (Medium) The relationship between climate hazards and social inequity is familiar to select city staff or elected \\n\\\n",
    "officials \\n\\\n",
    "4 (MidHigh) \\n\\\n",
    "5 (High) City staff and elected officials are well-versed in the concepts and taxonomy of the relationship between climate hazards and social inequity \\n\\\n",
    "6 I dont know \\n \"\n",
    "\n",
    "query += answers\n",
    "\n",
    "#human_template=\"\"\" {question} \"\"\"\n",
    "#human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd87f4-cff2-4d98-a205-afe633c51e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0a291-76be-404b-a155-4d7ebe9d2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query  = \"what are the key aspects of the orlando budget guide?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d51bc-9fcc-462a-9594-18162a1faf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = result.get(\"source_documents\", [])\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a262642c-6917-4772-a189-8f6f7f170172",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Run another question to see the results  ######\n",
    "#### Question 5d from the Orlando-PreAssessment\n",
    "\n",
    "query = \"Have potential barriers to the participation of vulnerable populations in \\\n",
    "the planning and implementation process been identified for the City Of Orlando? \\n\\n\"\n",
    "\n",
    "answers = \"Possible answers: \\n\\\n",
    "1 (Low) Potential barriers have not been studied or identified \\n\\\n",
    "2 (LowMid) \\n\\\n",
    "3 (Medium) Potential barriers have been identified, and plans to reduce barriers to participation are underway \\n\\\n",
    "4 (MidHigh) \\n\\\n",
    "5 (High) Barriers have been identified and the city has taken corrective action to reduce these barriers \\\n",
    "\"\n",
    "\n",
    "query += answers\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3bfc26-8891-4996-851d-556c437441c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2eca9-9e12-4173-81ce-9804abad1cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = result.get(\"source_documents\", [])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7796d7-489c-4551-8e82-16543b2bfb20",
   "metadata": {},
   "source": [
    "## Step 5 - Compare results with using a MultiQueryRetriever\n",
    "\n",
    "We are going to try to get even better results using a multi query retriiver, it may not affect the answer from multiple choice, but it will affect the explanation for the answer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f27656-9ebd-4a17-a8bb-97bb4da7e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\":6}), llm=llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d3d2b-338f-4b06-8086-cbe14e89cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Use the same query as above\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": chat_prompt\n",
    "    },\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98faaa-d9da-49b4-b7c6-e2b865a06b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = result.get(\"source_documents\", [])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc915fb5-2278-4eef-992e-cccb9344f1dc",
   "metadata": {},
   "source": [
    "## Step 6 - Go next level with Rank Fusion\n",
    "\n",
    "This is something taken from this repo originally: https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375556bf-e125-474a-a98a-60768a6deb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull('langchain-ai/rag-fusion-query-generation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e235d7-b71e-42ac-9a5a-24da3b77b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful assistant that generates multiple search queries based on a single input query.\"),\n",
    "#     (\"user\", \"Generate multiple search queries related to: {original_query}\"),\n",
    "#     (\"user\", \"OUTPUT (4 queries):\")\n",
    "# ])\n",
    "generate_queries = prompt | ChatOpenAI(temperature=0) | StrOutputParser() | (lambda x: x.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6072ba-d640-418b-93f9-6c929f29e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # Assumes the docs are returned in sorted order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "            \n",
    "    reranked_results = [(loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)]\n",
    "    return reranked_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7533d55-6120-4a40-838c-ef98112611f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = chain.invoke({\"original_query\": query})\n",
    "\n",
    "doc_list = []\n",
    "for d in docs:\n",
    "    for dd in d:\n",
    "        if hasattr(dd, 'page_content'):\n",
    "            doc_list.append(dd)\n",
    "     \n",
    "\n",
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e112bd7-3e94-45d7-aa47-9919793d4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_fusion_db = Chroma.from_documents(doc_list, embeddings)\n",
    "retriever=rank_fusion_db.as_retriever(search_kwargs={\"k\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c957301-f882-4d45-8b1a-5788bd7917cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Use the same query as above\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": chat_prompt\n",
    "    }, \n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ea232-0b4b-4f08-b5a5-ccba919dd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Run another question to see the results  ######\n",
    "#### Question 5a from the Orlando-PreAssessment\n",
    "\n",
    "query = \"To what extent are ECR-related priorities/projects coordinated with regional \\\n",
    "jurisdictions (e.g., city, county, state, districts, etc.)? \\n\"\n",
    "\n",
    "answers = \"Possible answers: \\n\\\n",
    "1 (Low)  The city does not coordinate with regional jurisdictions \\n\\\n",
    "2 (LowMid) \\n\\\n",
    "3 (Medium) The city informally coordinates with select regional jurisdictions \\n\\\n",
    "4 (MidHigh) \\n\\\n",
    "5 (High) The city has an established network and forum to coordinate with regional jurisdictions \\\n",
    "\"\n",
    "\n",
    "query += answers\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c21887-6c1b-49d0-914b-02d80c973bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = chain.invoke({\"original_query\": query})\n",
    "\n",
    "doc_list = []\n",
    "for d in docs:\n",
    "    for dd in d:\n",
    "        if hasattr(dd, 'page_content'):\n",
    "            doc_list.append(dd)\n",
    "     \n",
    "\n",
    "len(doc_list)\n",
    "\n",
    "rank_fusion_db = Chroma.from_documents(doc_list, embeddings)\n",
    "retriever=rank_fusion_db.as_retriever(search_kwargs={\"k\":10})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": chat_prompt\n",
    "    }, \n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d0063-96d2-4d4e-9fe6-176f2ae134b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dc7415-92f4-429a-ac25-bdebf06af15a",
   "metadata": {},
   "source": [
    "## More Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3601d31c-df08-448e-847e-23acfc694b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Run another question to see the results  ######\n",
    "#### Question 5b from the Orlando-PreAssessment\n",
    "\n",
    "query = \"Is there an established network of trusted agents to assist the city with reaching historically \\\n",
    "marginalized and/or vulnerable populations?? \\n\"\n",
    "\n",
    "answers = \"Possible answers: \\n\\\n",
    "1 (Low)  The city has not identified or utilized potential trusted agents to assist with community engagement activities \\n\\\n",
    "2 (LowMid) \\n\\\n",
    "3 (Medium) The city has identified and engaged with limited trusted agents to assist with community engagement activities \\n\\\n",
    "4 (MidHigh) \\n\\\n",
    "5 (High) The city has identified and utilized trusted agents to assist with all community engagement activities \\\n",
    "\"\n",
    "\n",
    "query += answers\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ed9b1-4ee6-4e62-8e5c-aa9ef2bb0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb161f-0496-42ca-ab80-1d112ad23f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
