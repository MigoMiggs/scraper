{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a644fc-9ef1-4616-88dd-7fc3ba2d6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import openai\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "import os\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = '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'\n",
    "print(os.environ['LANGCHAIN_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da21d7f-09d2-4f27-b4f7-bf4a240af39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "#embeddings_model_name = 'thenlper/gte-base'\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f884c-1fbc-4d25-ae34-6617834e261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"gpt-3.5-turbo-16k\"\n",
    "llm = ChatOpenAI(model_name=llm_model, temperature=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e7809-a575-45b9-8df4-50d9a9e43367",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = '../chroma_clean_ada/'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "\n",
    "print(vectordb._collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25182d1b-2d8f-4e69-b36c-1e63d595d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd4da9-4cfa-4487-8fc9-1b85dc1530c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")\n",
    "\"\"\"\n",
    "retriever=vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a42311-ecbc-4a38-b82a-fd14e742408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_question = \"I will give you a multiple choice question and \\\n",
    "you will pick the right answer based on your knowledge and the given context. \\\n",
    "The sixth answer is always I don't know. \\\n",
    "You will pick the answer from the multiple choices presented. \\\n",
    "Also, after the answer, you will explain how you got to the answer, referrring to the pieces \\\n",
    "of context that gave you the answer: \\\n",
    "\\\n",
    "<question> \\\n",
    "{question} \\\n",
    "\\\n",
    "<possible answers> \\\n",
    "{answers}\\\n",
    "\"\n",
    "\n",
    "meta_prompt = PromptTemplate(\n",
    "    template = meta_question, input_variables=['question', 'answers']\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e120ce-ab8e-4c99-b199-32207b7ed74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d6c34-7053-4f3d-8e61-78a7836df666",
   "metadata": {},
   "outputs": [],
   "source": [
    "query  = \"what are the key aspects of the orlando budget guide?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc84bec-384f-4dc4-aa49-a39094631836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Question 1A from the Orlando-PreAssessment\n",
    "\n",
    "question = \"To what extent is the relationship between climate hazards and \\\n",
    "social vulnerability/inequity understood among city leaders and staff?\"\n",
    "\n",
    "answers = \" \\\n",
    "1 (Low) The relationship between climate hazards and social inequity has not been explored by staff or elected officials \\\n",
    "2 (LowMid) \\\n",
    "3 (Medium) The relationship between climate hazards and social inequity is familiar to select city staff or elected \\\n",
    "officials \\\n",
    "4 (MidHigh) \\\n",
    "5 (High) City staff and elected officials are well-versed in the concepts and taxonomy of the relationship between climate hazards and social inequity \\\n",
    "6 I dont know \\\n",
    "\"\n",
    "\n",
    "query = meta_prompt.format(question=question, answers=answers)\n",
    "query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc23a2e-98fc-4de3-807f-a87342521ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49a199e-3545-41f9-b342-8f52a2e32626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Question 1d from the Orlando-PreAssessment\n",
    "\n",
    "question = \"Does the city of Orlando have stated Equitable Climate Resilience (ECR) goals?\"\n",
    "\n",
    "answers = \" \\\n",
    "1 (Low) The city has not set out to establish goals for ECR in the community \\\n",
    "2 (LowMid) \\\n",
    "3 (Medium) The city has conducted engagement activities with \\\n",
    "community leaders to facilitate ECR goal setting \\\n",
    "4 (MidHigh) \\\n",
    "5 (High) The city has articulated a set of tangible ECR goals \\\n",
    "6 I dont know \\\n",
    "\"\n",
    "\n",
    "query = meta_prompt.format(question=question, answers=answers)\n",
    "query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab94f2-ef93-4536-acbe-605296e3b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa9c92-ef0e-4f74-ac6e-4accc16d7dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Question 4a from the Orlando-PreAssessment\n",
    "\n",
    "question = \"Is there an advisory board/task force/working group, in the City of \\\n",
    "Orlando to facilitate the periodic \\\n",
    "analysis of climate hazards and impacts on vulnerable populations?\"\n",
    "\n",
    "answers = \" \\\n",
    "1 (Low) There is no advisory board/task force/working group to inform the city’s efforts on climate and equity \\\n",
    "2 (LowMid) \\\n",
    "3 (Medium) There is an internal working group/advisory board/task force to inform the city’s \\\n",
    "efforts on climate and equity \\\n",
    "4 (MidHigh) \\\n",
    "5 (High) There is a working group/advisory board/task force the consists of internal \\\n",
    "and external stakeholders to inform the city’s efforts on climate and equity \\\n",
    "\"\n",
    "query = meta_prompt.format(question=question, answers=answers)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3232df8-b53c-4464-9aaf-923b00df695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b8697-0947-49e1-b62f-a3f79d5229cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Question 5d from the Orlando-PreAssessment\n",
    "\n",
    "question = \"Have potential barriers to the participation of vulnerable populations in \\\n",
    "the planning and implementation process been identified for the City Of Orlando?\"\n",
    "\n",
    "answers = \" \\\n",
    "1 (Low) Potential barriers have not been studied or identified \\\n",
    "2 (LowMid) \\\n",
    "3 (Medium) Potential barriers have been identified, and plans to reduce barriers to participation are underway \\\n",
    "4 (MidHigh) \\\n",
    "5 (High)  Barriers have been identified and the city has taken corrective action to reduce these barriers \\\n",
    "\"\n",
    "query = meta_prompt.format(question=question, answers=answers)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fc8b1-d8ca-4962-8777-1258ce087a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12804ae-d781-4f9e-9828-45a27f0aca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Question 5f from the Orlando-PreAssessment\n",
    "\n",
    "question = \"Are community engagement activities tracked to determine performance and desired outcomes in the city of Orlando?\"\n",
    "\n",
    "answers = \" \\\n",
    "1 (Low) The city has not established any metrics to measure success of engagement activities \\\n",
    "2 (LowMid) \\\n",
    "3 (Medium) The city has established metrics to measure success of engagement activities but \\\n",
    "has not utilized the data to inform future activities \\\n",
    "4 (MidHigh) \\\n",
    "5 (High) The city has established metrics to measure success of engagement activities, \\\n",
    "has made the data available to the public, and has utilized the data to inform future activities \\\n",
    "\"\n",
    "query = meta_prompt.format(question=question, answers=answers)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd8439-ca82-4278-8084-81d2241f4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d532ea4-1ff0-41ed-a5d4-521a17125e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
